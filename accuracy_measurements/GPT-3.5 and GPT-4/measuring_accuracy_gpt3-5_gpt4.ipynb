{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to preprocess the data and evaluate the GPT-3-5 and GPT-4 models on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import setup\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=setup.openai_api_key, organization=setup.openai_organization_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(folder_name):\n",
    "  # extract the data\n",
    "  folder_names = sorted(os.listdir(folder_name))\n",
    "  # put all text in a list\n",
    "  text = []\n",
    "  for folder in folder_names:\n",
    "    with open(folder_name + '/' + folder + '/text.json') as file:\n",
    "      file_content = file.read()\n",
    "      file_content_str = json.loads(file_content)\n",
    "      # need to convert the file content to a dict to extract text\n",
    "      file_content_dict = json.loads(file_content_str)\n",
    "      text.append(file_content_dict[\"text\"])\n",
    "  return text\n",
    "\n",
    "def clean_text(text):\n",
    "  # clean the data\n",
    "  text_prepros = []\n",
    "  for ex in text:\n",
    "    new = re.sub('\\n.+\\|thumb.*\\n?(.*)?', '', ex)\n",
    "    new = re.sub('\\n?(.*)?\\|thumb.*\\n', '', new)\n",
    "    new = re.sub('\\n.+thumb\\|.*\\n?(.*)?', '', new)\n",
    "    new = re.sub('\\n?(.*)?thumb\\|.*\\n', '', new)\n",
    "    new = re.sub('\\n.+\\|thumbnail.*\\n?(.*)?', '', new)\n",
    "    new = re.sub('\\n?(.*)?\\|thumbnail.*\\n', '', new)\n",
    "    new = re.sub('\\n.+thumbnail\\|.*\\n?(.*)?', '', new)\n",
    "    new = re.sub('\\n?(.*)?thumbnail\\|.*\\n', '', new)\n",
    "    new = new.replace('\\\"', '')\n",
    "    new = new.replace('\"\\\"', '')\n",
    "    new = html.unescape(new)\n",
    "    new = new.encode('ascii', 'ignore').decode()\n",
    "    new = re.sub(r'Accessed.*?\\.', '', new)\n",
    "    new = new.replace('  ', ' ')\n",
    "    new = re.sub(r'(http|www)\\S+', '', new)\n",
    "    new = re.sub(r'External Links.*', '', new, flags=re.DOTALL)\n",
    "    new = re.sub(r'References.*', '', new, flags=re.DOTALL)\n",
    "    if '\\nthumb |upright=1.35 |Centres of origin, as numbered by Nikolai Vavilov in the 1930s. Area 3 (gray) is no longer recognised as a centre of origin, and Papua New Guinea (area P, orange) was identified more recently.\\n' in new:\n",
    "      new = new.replace('\\nthumb |upright=1.35 |Centres of origin, as numbered by Nikolai Vavilov in the 1930s. Area 3 (gray) is no longer recognised as a centre of origin, and Papua New Guinea (area P, orange) was identified more recently.\\n', '')\n",
    "    if '\\n375px|Deprotonation equilibrium of acetic acid in water' in new:\n",
    "      new = new.replace('\\n375px|Deprotonation equilibrium of acetic acid in water', '')\n",
    "    text_prepros.append(new)\n",
    "  return text_prepros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text(\"wikipedia_data_v3\")\n",
    "text_clean = clean_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(enc, text_clean):\n",
    "    # tokenize the text\n",
    "    tokenized = []\n",
    "    for ex in text_clean:\n",
    "        encoding = enc.encode(ex)\n",
    "        tokenized.append(encoding)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_accuracy(example, filepath1_context, filepath2_topp, filepath3_predictions, enc, model_name):\n",
    "    # make model predictions and obtain accuracy measurements\n",
    "    with open(filepath1_context, 'w') as file:\n",
    "        json.dump({}, file)\n",
    "    with open(filepath2_topp, 'w') as file:\n",
    "        json.dump({}, file)\n",
    "    with open(filepath3_predictions, 'w') as file:\n",
    "        json.dump({}, file)\n",
    "    total_acc_p1, total_acc_p2, total_acc_p3, total_acc_p4, total_acc_p5 = 0, 0, 0, 0, 0\n",
    "    context_lengths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000]\n",
    "    for c in context_lengths:\n",
    "        example_dict = {}\n",
    "        top_p_predictions = {}\n",
    "        predictions_dict = {}\n",
    "        context_tokens = example[0:c]\n",
    "        context = enc.decode(context_tokens)\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": context}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=1,\n",
    "            logprobs=True,\n",
    "            top_logprobs=5\n",
    "        )\n",
    "        correct_string = enc.decode([example[c]])\n",
    "        stripped_correct_string = correct_string.strip()\n",
    "\n",
    "        predictions = response.choices[0].logprobs.content[0].top_logprobs\n",
    "        \n",
    "        probs = []\n",
    "        top_tokens = []\n",
    "        # get probabilities and tokens in order\n",
    "        for i in predictions:\n",
    "            probs.append(i.logprob)\n",
    "            top_tokens.append(i.token)\n",
    "        predictions_dict[\"tokens\"] = top_tokens\n",
    "        predictions_dict[\"probabilities\"] = probs\n",
    "        if probs == sorted(probs, reverse=True):\n",
    "            for e in range(0, len(predictions)):\n",
    "                top_p_predictions['token_top_' + str(e+1)] = predictions[e].token\n",
    "                top_p_predictions['prob_top_' + str(e+1)] = predictions[e].logprob\n",
    "                if stripped_correct_string in top_tokens[0:e+1] or correct_string in top_tokens[0:e+1]:\n",
    "                    top_p_predictions['accuracy_top_' + str(e+1)] = 1\n",
    "                    if e + 1 == 1:\n",
    "                        total_acc_p1 += 1\n",
    "                    elif e + 1 == 2:\n",
    "                        total_acc_p2 += 1\n",
    "                    elif e + 1 == 3:\n",
    "                        total_acc_p3 += 1\n",
    "                    elif e + 1 == 4:\n",
    "                        total_acc_p4 += 1\n",
    "                    elif e + 1 == 5:\n",
    "                        total_acc_p5 += 1\n",
    "                else:\n",
    "                    top_p_predictions['accuracy_top_' + str(e+1)] = 0\n",
    "        else:\n",
    "            together = sorted(zip(probs, top_tokens), reverse=True)\n",
    "            probs, top_tokens = zip(*together)\n",
    "            for e in range(0, len(probs)):\n",
    "                top_p_predictions['token_top_' + str(e+1)] = top_tokens[e]\n",
    "                top_p_predictions['prob_top_' + str(e+1)] = probs[e]\n",
    "                if stripped_correct_string in top_tokens[0:e+1] or correct_string in top_tokens[0:e+1]:\n",
    "                    top_p_predictions['accuracy_top_' + str(e+1)] = 1\n",
    "                    if e + 1 == 1:\n",
    "                        total_acc_p1 += 1\n",
    "                    elif e + 1 == 2:\n",
    "                        total_acc_p2 += 1\n",
    "                    elif e + 1 == 3:\n",
    "                        total_acc_p3 += 1\n",
    "                    elif e + 1 == 4:\n",
    "                        total_acc_p4 += 1\n",
    "                    elif e + 1 == 5:\n",
    "                        total_acc_p5 += 1\n",
    "                else:\n",
    "                    top_p_predictions['accuracy_top_' + str(e+1)] = 0\n",
    "        top_p_predictions['token_correct_stripped'] = stripped_correct_string\n",
    "        top_p_predictions['token_correct_not_stripped'] = correct_string\n",
    "\n",
    "        gpt_output = top_tokens[0]\n",
    "        accuracy_string = 0\n",
    "        if stripped_correct_string == gpt_output or correct_string == gpt_output:\n",
    "            accuracy_string = 1\n",
    "\n",
    "        example_dict[\"context\"] = context\n",
    "        example_dict[\"correct_string_not_stripped\"] = correct_string\n",
    "        example_dict[\"correct_token_not_stripped\"] = example[c]\n",
    "        example_dict[\"correct_string_stripped\"] = stripped_correct_string\n",
    "        example_dict[\"gpt_predicted_string\"] = gpt_output\n",
    "        example_dict[\"accuracy_string\"] = accuracy_string\n",
    "\n",
    "        # save results\n",
    "        with open(filepath1_context, \"r\") as file:\n",
    "            current = json.load(file)\n",
    "        current.update({c: example_dict})\n",
    "        with open(filepath1_context, \"w\") as file:\n",
    "            json.dump(current, file)\n",
    "\n",
    "        with open(filepath2_topp, \"r\") as file:\n",
    "            current = json.load(file)\n",
    "        current.update({c: top_p_predictions})\n",
    "        with open(filepath2_topp, \"w\") as file:\n",
    "            json.dump(current, file)\n",
    "\n",
    "        with open(filepath3_predictions, \"r\") as file:\n",
    "            current = json.load(file)\n",
    "        current.update({c: predictions_dict})\n",
    "        with open(filepath3_predictions, \"w\") as file:\n",
    "            json.dump(current, file)\n",
    "\n",
    "    return total_acc_p1/len(context_lengths), total_acc_p2/len(context_lengths), total_acc_p3/len(context_lengths), total_acc_p4/len(context_lengths), total_acc_p5/len(context_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which model to use for encoding\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo-0125\")\n",
    "# enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "# enc = tiktoken.encoding_for_model(\"gpt-4-0125-preview\")\n",
    "\n",
    "# select which model to use\n",
    "model_name = \"gpt-3.5-turbo-0125\"\n",
    "# model_name = \"gpt-4\"\n",
    "# model_name = \"gpt-4-0125-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulty_tensor_p1 = torch.zeros(100)\n",
    "difficulty_tensor_p2 = torch.zeros(100)\n",
    "difficulty_tensor_p3 = torch.zeros(100)\n",
    "difficulty_tensor_p4 = torch.zeros(100)\n",
    "difficulty_tensor_p5 = torch.zeros(100)\n",
    "# change output filenames according to model\n",
    "torch.save(difficulty_tensor_p1, 'gpt3-5_100ex_p1_diff.pt')\n",
    "torch.save(difficulty_tensor_p2, 'gpt3-5_100ex_p2_diff.pt')\n",
    "torch.save(difficulty_tensor_p3, 'gpt3-5_100ex_p3_diff.pt')\n",
    "torch.save(difficulty_tensor_p4, 'gpt3-5_100ex_p4_diff.pt')\n",
    "torch.save(difficulty_tensor_p5, 'gpt3-5_100ex_p5_diff.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the accuracy measurements on all examples\n",
    "# change filenames according to model\n",
    "difficulty_tensor_p1 = torch.load('gpt3-5_100ex_p1_diff.pt')\n",
    "difficulty_tensor_p2 = torch.load('gpt3-5_100ex_p2_diff.pt')\n",
    "difficulty_tensor_p3 = torch.load('gpt3-5_100ex_p3_diff.pt')\n",
    "difficulty_tensor_p4 = torch.load('gpt3-5_100ex_p4_diff.pt')\n",
    "difficulty_tensor_p5 = torch.load('gpt3-5_100ex_p5_diff.pt')\n",
    "tokens = tokenize(enc, text_clean)\n",
    "for i in range(0, 100):\n",
    "    ex = tokens[i]\n",
    "    # change filenames according to model\n",
    "    accuracy_p1, accuracy_p2, accuracy_p3, accuracy_p4, accuracy_p5 = measure_accuracy(ex, 'output_context_gpt35/gpt35context_ex' + str(i) + '.json', 'output_accuracies_gpt35/gpt35topp_ex' + str(i) + '.json', 'output_predictions_gpt35/gpt35pred_ex' + str(i) + '.json', enc, model_name)\n",
    "    difficulty_tensor_p1[i] = accuracy_p1\n",
    "    difficulty_tensor_p2[i] = accuracy_p2\n",
    "    difficulty_tensor_p3[i] = accuracy_p3\n",
    "    difficulty_tensor_p4[i] = accuracy_p4\n",
    "    difficulty_tensor_p5[i] = accuracy_p5\n",
    "# change filenames according to model\n",
    "torch.save(difficulty_tensor_p1, 'gpt3-5_100ex_p1_diff.pt')\n",
    "torch.save(difficulty_tensor_p2, 'gpt3-5_100ex_p2_diff.pt')\n",
    "torch.save(difficulty_tensor_p3, 'gpt3-5_100ex_p3_diff.pt')\n",
    "torch.save(difficulty_tensor_p4, 'gpt3-5_100ex_p4_diff.pt')\n",
    "torch.save(difficulty_tensor_p5, 'gpt3-5_100ex_p5_diff.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
